{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import logging\n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, OpenAIAdam, cached_path\n",
    "#from torchnlp.metrics import get_moses_multi_bleu\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
    "from bertviz.bertviz.pytorch_pretrained_bert import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<POS>', '<NEG>','<CON_START>','<START>','<END>'] # Set the special tokens\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('./openai_gpt_vocab/', special_tokens=special_tokens)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # torch.device(\"cuda:1\")\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt', num_special_tokens=len(special_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GST Model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_bgst_model_path = \"PATH of PRE-TRAINED BGST MODEL\"\n",
    "model_state_dict = torch.load(yelp_bgst_model_path)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Decoding Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len=120\n",
    "sm = torch.nn.Softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(ref_text, p=0.9):\n",
    "    \"\"\"\n",
    "    This functions decodes the sentence by sampling from the samples whose culilitive probability\n",
    "    is greater of equal to p.\n",
    "    \"\"\"\n",
    "    sm = torch.nn.Softmax(dim=-1) # To calculate Softmax over the final layer Logits\n",
    "    tokens = tokenizer.tokenize(ref_text) # Tokenize the input text\n",
    "    \n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens) # Convert tokens to ids\n",
    "\n",
    "    #index_tokens = [indexed_tokens for i in range(beam_width)]\n",
    "    torch_tensor = torch.tensor(indexed_tokens).unsqueeze(0).to(device)\n",
    "    current_state = torch.tensor(indexed_tokens).unsqueeze(0).to(device)\n",
    "    #print(current_state)\n",
    "    decoded_indexes = []\n",
    "    count = 0\n",
    "    stop_decode = False\n",
    "    #while count < model.config.n_positions and not stop_decode:\n",
    "    while count < 512 - len(tokens) and not stop_decode and count < len(tokens)+5:\n",
    "        #print(\"Torch Tensor = {}\".format(current_state))\n",
    "        with torch.no_grad():\n",
    "            preds = sm(model(current_state))\n",
    "        sort_v, sort_i = torch.sort(preds[:,-1,:], descending=True)\n",
    "        current_p = 0\n",
    "        for i,x in enumerate(sort_v[0]):\n",
    "            current_p += x\n",
    "            #print(current_p)\n",
    "            if current_p > p:\n",
    "                final_v = sort_v[0][:i+1]\n",
    "                final_i = sort_i[0][:i+1]\n",
    "                break\n",
    "        cat = torch.distributions.Categorical(final_v)\n",
    "        sampled_index = cat.sample().item()\n",
    "        #print(\"Final_v = {}\".format(final_v))\n",
    "        decoded_indexes.append(final_i.tolist()[sampled_index])\n",
    "        current_state = torch.cat( (torch_tensor, torch.tensor(decoded_indexes).unsqueeze(0).to(device)), dim=1)\n",
    "        count += 1\n",
    "        if decoded_indexes[-1] == tokenizer.special_tokens[\"<END>\"]:\n",
    "            stop_decode = True\n",
    "            \n",
    "    try:\n",
    "        end_index = decoded_indexes.index(tokenizer.special_tokens[\"<END>\"])\n",
    "    except ValueError:\n",
    "        end_index = len(decoded_indexes)\n",
    "    decoded_sentences = tokenizer.decode(decoded_indexes[:end_index])\n",
    "    training_inputs = indexed_tokens + decoded_indexes[:end_index]  \n",
    "    \n",
    "    return decoded_sentences, decoded_indexes[:end_index], training_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_decoding(ref_text, samples=5):\n",
    "    decoded_sentences = [None] * samples\n",
    "    decoded_indexes = [None] * samples\n",
    "    training_inputs = [None] * samples\n",
    "    \n",
    "    for k in range(samples):\n",
    "        decoded_sentences[k], decoded_indexes[k], training_inputs[k] = top_p_sampling(ref_text)\n",
    "    return decoded_sentences, decoded_indexes, training_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preditction_with_beam_search(ref_text, beam_width=5, vocab_length=40483):\n",
    "    \"\"\"\n",
    "    This function decodes sentences using Beam Seach. \n",
    "    It will output #sentences = beam_width. This function works on a single example.\n",
    "    \n",
    "    ref_text : string : Input sentence\n",
    "    beam_width : int : Width of the output beam\n",
    "    vocab_length : int : Size of the Vocab after adding the special tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    done = [False for i in range(beam_width)] # To stop decoding for sentences after <END> token\n",
    "    stop_decode = False\n",
    "    decoded_sentences=[] # List of decoded sentences at any given time\n",
    "    \n",
    "    sm = torch.nn.Softmax(dim=-1) # To calculate Softmax over the final layer Logits\n",
    "    tokens = tokenizer.tokenize(ref_text) # Tokenize the input text\n",
    "    \n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens) # Convert tokens to ids\n",
    "    index_tokens = [indexed_tokens for i in range(beam_width)] # Replication of Input ids for all the beams\n",
    "\n",
    "    #index_tokens = [indexed_tokens for i in range(beam_width)]\n",
    "    torch_tensor = torch.tensor(index_tokens).to(device)\n",
    "    beam_indexes = [[] for i in range(beam_width)] # indexes of the current decoded beams\n",
    "    best_scoes = [0 for i in range(beam_width)] # A list of lists to store Probability values of each decoded token of best beams\n",
    "    count = 0\n",
    "    #print(\"Tokens = {}\".format(len(tokens)))\n",
    "    while count < model.config.n_positions - len(tokens) and not stop_decode and count < len(tokens)+5:\n",
    "        \n",
    "        if count == 0: # For the first step when only one sentence is availabe\n",
    "            with torch.no_grad():\n",
    "                # Calculate output probability distribution over the Vocab,\n",
    "                model.eval()\n",
    "                preds = sm(model(torch_tensor)) #  shape = [beam_bidth, len(input_sen)+1,Vocab_length]\n",
    "            top_v, top_i = preds[:,-1,:].topk(beam_width) # Fatch top indexes and it's values\n",
    "            [beam_indexes[i].append(top_i[0][i].tolist()) for i in range(beam_width)] # Update the Beam indexes\n",
    "            # Update the best_scores, for first time just add the topk values directly\n",
    "            for i in range(beam_width):\n",
    "                best_scoes[i] = top_v[0][i].item()\n",
    "            count += 1\n",
    "        else: # After first step\n",
    "            # Prepare the current_state by concating original input and decoded beam indexes\n",
    "            current_state = torch.cat((torch_tensor, torch.tensor(beam_indexes).to(device)), dim=1)\n",
    "            # Prediction on the current state\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                preds = sm(model(current_state))\n",
    "            # Multiply new probability predictions with corresponding best scores\n",
    "            # Total socres = beam_width * Vocab_Size\n",
    "            flatten_score = (preds[:,-1,:]*torch.tensor(best_scoes).to(device).unsqueeze(1)).view(-1)\n",
    "            # Fatch the top scores and indexes \n",
    "            vals, inx = flatten_score.topk(beam_width)\n",
    "            # best_score_inx saves the index of best beams after multiplying the probability of new prediction\n",
    "            best_scoes_inx = (inx / vocab_length).tolist()\n",
    "            best_scoes = vals.tolist()\n",
    "            # Unflatten the index \n",
    "            correct_inx = (inx % vocab_length).tolist()\n",
    "            \n",
    "            # Check if done for all the Beams\n",
    "            for i in range(beam_width):\n",
    "                if correct_inx[i] == tokenizer.special_tokens[\"<END>\"]:\n",
    "                    done[i] = True\n",
    "            # Update the best score for each the current Beams\n",
    "            for i in range(beam_width):\n",
    "                if not done[i]:\n",
    "                    best_scoes[i] = vals.tolist()[i]\n",
    "            # Check is All the Beams are Done\n",
    "            if (sum(done) == beam_width):\n",
    "                stop_decode = True\n",
    "            # Prepapre the new beams\n",
    "            temp_lt=[0 for i in range(beam_width)]\n",
    "            for i,x in enumerate(best_scoes_inx):\n",
    "                temp_lt[i] = beam_indexes[x] + [correct_inx[i]]\n",
    "            # Update the Beam indexes\n",
    "            beam_indexes = temp_lt\n",
    "            del temp_lt\n",
    "            count += 1\n",
    "    # Decode All the beam indexes to sentences by removing the \"<END>\" token\n",
    "    training_inputs = [None for i in range(beam_width)]\n",
    "    for i in range(beam_width):\n",
    "        \n",
    "        try:\n",
    "            end_index = beam_indexes[i].index(tokenizer.special_tokens[\"<END>\"])\n",
    "        except ValueError:\n",
    "            end_index = len(beam_indexes[i])\n",
    "        training_inputs[i] = index_tokens[i] + beam_indexes[i][:end_index]\n",
    "        \n",
    "        decoded_sentences.append(tokenizer.decode(beam_indexes[i][:end_index]))\n",
    "        beam_indexes[i] = beam_indexes[i][:end_index]\n",
    "        \n",
    "    return decoded_sentences, beam_indexes, training_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_training_samples = 2000\n",
    "np.random.seed = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment0_path = \"Path of sentiment0 (Negative) training data\"\n",
    "train_sentiment1_path = \"Path of sentiment0 (Positive) training data\"\n",
    "with open(train_sentiment0_path) as fp:\n",
    "    train0 = fp.read().splitlines()\n",
    "with open(train_sentiment1_path) as fp:\n",
    "    train1 = fp.read().splitlines()\n",
    "    \n",
    "train0_indexes = np.random.choice(len(train0), rl_training_samples//2, replace=False)\n",
    "train1_indexes = np.random.choice(len(train1), rl_training_samples//2, replace=False)\n",
    "\n",
    "train0 = [train0[z1] for z1 in train0_indexes]\n",
    "train1 = [train1[z1] for z1 in train1_indexes]\n",
    "\n",
    "train0 = [\"<CON_START> {} <START>\".format(x) for x in train0]\n",
    "train1 = [\"<CON_START> {} <START>\".format(x) for x in train1]\n",
    "\n",
    "train = train0 + train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\"<POS>\",\"<NEG>\"]\n",
    "train = [t + \" \" + x for x in train0 + train1 for t in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0 = [x.replace(\"<POS> \",\"\").replace(\"<CON_START> \",\"\").replace(\" <START>\",\"\") for x in train0]\n",
    "r1 = [x.replace(\"<NEG> \",\"\").replace(\"<CON_START> \",\"\").replace(\" <START>\",\"\") for x in train1]\n",
    "r = r0 + r1\n",
    "r[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r_for_bleu = [x1 for x1 in r for k1 in range(10*2)]\n",
    "r_for_bleu = [x1 for x1 in r for k1 in range(10)]\n",
    "len(r_for_bleu), r_for_bleu[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls = [1,0]\n",
    "labls = [l for i in range(len(train0) + len(train1)) for l in lbls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labls[:10], len(labls), len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Feedback classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_CLASSIFIER_PATH = \"path of bert classifier\"\n",
    "model_cls = BertForSequenceClassification.from_pretrained(BERT_CLASSIFIER_PATH, num_labels=2)\n",
    "tokenizer_cls = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "model_cls.to(device)\n",
    "model_cls.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_examples(input_sentences, bs=32, dvc='cpu'):\n",
    "    \"\"\"\n",
    "    To evaluate whole dataset and return predictions\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    segment_ids = []\n",
    "    input_masks = []\n",
    "    pred_lt = []\n",
    "    pred_value = []\n",
    "    for sen in input_sentences:\n",
    "        text_tokens = tokenizer_cls.tokenize(sen)[:max_seq_len-2]\n",
    "        tokens = [\"[CLS]\"] + text_tokens + [\"[SEP]\"]\n",
    "        temp_ids = tokenizer_cls.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(temp_ids)\n",
    "        segment_id = [0] * len(temp_ids)\n",
    "        padding = [0] * (max_seq_len - len(temp_ids))\n",
    "\n",
    "        temp_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_id += padding\n",
    "        \n",
    "        ids.append(temp_ids)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "    \n",
    "    ids = torch.tensor(ids) # .to('cuda')\n",
    "    segment_ids = torch.tensor(segment_ids) # .to('cuda')\n",
    "    input_masks = torch.tensor(input_masks) # .to('cuda')\n",
    "    \n",
    "    steps = len(ids) // bs\n",
    "    \n",
    "    for i in trange(steps+1):\n",
    "        if i == steps:\n",
    "            temp_ids = ids[i * bs : len(ids)]\n",
    "            temp_segment_ids = segment_ids[i * bs: len(ids)]\n",
    "            temp_input_masks = input_masks[i * bs: len(ids)]\n",
    "        else:\n",
    "            temp_ids = ids[i * bs : i * bs + bs]\n",
    "            temp_segment_ids = segment_ids[i * bs: i * bs + bs]\n",
    "            temp_input_masks = input_masks[i * bs: i * bs + bs]\n",
    "        \n",
    "        temp_ids = temp_ids.to(dvc)\n",
    "        temp_segment_ids = temp_segment_ids.to(dvc)\n",
    "        temp_input_masks = temp_input_masks.to(dvc)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = sm(model_cls(temp_ids, temp_segment_ids, temp_input_masks))\n",
    "        \n",
    "        try:\n",
    "            ps = torch.argmax(preds, dim=-1)\n",
    "            pred_lt.extend(ps.tolist())\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        pred_value.extend(preds.tolist())\n",
    "    \n",
    "    pred_value1 = [z2[pred_lt[k5]] for k5,z2 in enumerate(pred_value)]\n",
    "    return pred_lt, pred_value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = train\n",
    "input_data_label = labls\n",
    "r_for_bleu = r_for_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT model for token level feedback for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(BERT_CLASSIFIER_PATH)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "bert_model.to(device)\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_attn_examples(input_sentences, dvc='cpu',bs=32, head=7):\n",
    "    \"\"\"\n",
    "    To evaluate whole dataset and return predictions\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    ids_to_decode = [None for k in range(len(input_sentences))]\n",
    "    tokens_to_decode = [None for k in range(len(input_sentences))]\n",
    "    segment_ids = []\n",
    "    input_masks = []\n",
    "    attention_weights = [None for z in input_sentences]\n",
    "    for j,sen in enumerate(tqdm(input_sentences)):\n",
    "        sen = sen..replace(\" ' \",\"'\").replace(\"ca n't\", \"can not\").replace(\"wo n't\",\"will not\").replace(\"n't\", \" not\")\n",
    "        text_tokens = bert_tokenizer.tokenize(sen)[:max_seq_len-5]\n",
    "        tokens = [\"[CLS]\"] + text_tokens + [\"[SEP]\"]\n",
    "        tokens_to_decode[j] = tokens + ['[PAD]']\n",
    "        temp_ids = bert_tokenizer.convert_tokens_to_ids(tokens) + [0]\n",
    "        ids_to_decode[j] = temp_ids\n",
    "        input_mask = [1] * len(temp_ids)\n",
    "        segment_id = [0] * len(temp_ids)\n",
    "        padding = [0] * (max_seq_len - len(temp_ids))\n",
    "        \n",
    "        \n",
    "        temp_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_id += padding\n",
    "        \n",
    "        ids.append(temp_ids)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "    \n",
    "    ids = torch.tensor(ids) # .to('cuda')\n",
    "    segment_ids = torch.tensor(segment_ids) #.to('cuda')\n",
    "    input_masks = torch.tensor(input_masks) #.to('cuda')\n",
    "    \n",
    "    steps = len(ids) // bs\n",
    "    \n",
    "    for i in trange(steps+1):\n",
    "        if i == steps:\n",
    "            temp_ids = ids[i * bs : len(ids)]\n",
    "            temp_segment_ids = segment_ids[i * bs: len(ids)]\n",
    "            temp_input_masks = input_masks[i * bs: len(ids)]\n",
    "        else:\n",
    "            temp_ids = ids[i * bs : i * bs + bs]\n",
    "            temp_segment_ids = segment_ids[i * bs: i * bs + bs]\n",
    "            temp_input_masks = input_masks[i * bs: i * bs + bs]\n",
    "        \n",
    "        temp_ids = temp_ids.to(dvc)\n",
    "        temp_segment_ids = temp_segment_ids.to(dvc)\n",
    "        temp_input_masks = temp_input_masks.to(dvc)\n",
    "        with torch.no_grad():\n",
    "             _, _, attn = bert_model(temp_ids, temp_segment_ids, temp_input_masks)\n",
    "        \n",
    "        for j in range(len(attn[9]['attn_probs'])):\n",
    "            attention_weights[i * bs + j] = (attn[9]['attn_probs'][j][head][0]).to('cpu')\n",
    "    \n",
    "    return attention_weights, ids_to_decode, tokens_to_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words=['is','are','was','were','has','have','had','a','an','the','this','that','these','those','there','how','i','we',\n",
    "             'he','she','it','they','them','their','his','him','her','us','our', 'and','in','my','your','you', 'will', 'shall']\n",
    "common_words_tokens = bert_tokenizer.convert_tokens_to_ids(common_words)\n",
    "not_to_remove_ids = bert_tokenizer.convert_tokens_to_ids([\"[CLS]\",\"[SEP]\", \".\", \"?\", \"!\"])\n",
    "not_to_remove_ids += common_words_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(aw, ids_to_decode, tokens_to_decode, threshold=0.5):\n",
    "    out_sen = [None for i in range(len(aw))]\n",
    "    for i in trange(len(aw)):\n",
    "        #topv, topi = aw[i].topk(len(inps_tokens[i]))\n",
    "        topv, topi = aw[i].topk(ids_to_decode[i].index(0))\n",
    "        topi = topi.tolist()\n",
    "        topv = topv.tolist()\n",
    "        \n",
    "        #print(\"Original Top Indexes = {}\".format(topi))\n",
    "        topi = [topi[j] for j in range(len(topi)) if ids_to_decode[i][topi[j]] not in not_to_remove_ids] # remove noun and common words\n",
    "        #print(\"After removing Nouns = {}\".format(topi))\n",
    "        topi = [topi[j] for j in range(len(topi)) if \"##\" not in tokens_to_decode[i][topi[j]]] # Remove half words\n",
    "        #print(\"After removing Half-words = {}\".format(topi))\n",
    "        \n",
    "        topi = topi[:int(threshold * len(topi))]\n",
    "        #print(\"Final Topi = {}\".format(topi))\n",
    "        final_indexes = []\n",
    "        count = 0\n",
    "        count1 = 0\n",
    "        #print(ids_to_decode[i], tokens_to_decode[i])\n",
    "        while ids_to_decode[i][count] != 0:\n",
    "            if count in topi:\n",
    "                while ids_to_decode[i][count + count1 + 1] != 0:\n",
    "                    if \"##\" in tokens_to_decode[i][count + count1 + 1]:\n",
    "                        count1 += 1\n",
    "                    else:\n",
    "                        break\n",
    "                count += count1\n",
    "                count1 = 0\n",
    "            else:\n",
    "                final_indexes.append(ids_to_decode[i][count])\n",
    "            count += 1\n",
    "\n",
    "        #print(final_indexes)\n",
    "        temp_out_sen = tokenizer.convert_ids_to_tokens(final_indexes)\n",
    "        temp_out_sen = \" \".join(temp_out_sen).replace(\" ##\", \"\").replace(\"[CLS]\",\"\").replace(\"[SEP]\",\"\")\n",
    "        #print(temp_out_sen, \"\\n\\n\")\n",
    "        out_sen[i] = temp_out_sen.strip()\n",
    "    \n",
    "    return out_sen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LM for Perplexity feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model_special_tokens = [\"<POS>\",\"<NEG>\",\"<END>\"]\n",
    "lm_tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt', special_tokens=lm_model_special_tokens)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lm_model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt', num_special_tokens=len(lm_model_special_tokens))\n",
    "\n",
    "openai_gpt_lm_path = \"PATH OF OPENAI GPT MODEL\"\n",
    "lm_model_state_dict = torch.load(openai_gpt_lm_path)\n",
    "lm_model.load_state_dict(lm_model_state_dict)\n",
    "lm_model.to(device)\n",
    "lm_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ppl(tokenized_ids, bs=32, dvc='cpu'):\n",
    "    \n",
    "    lm_loss = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n",
    "    # tokenize the sentences\n",
    "    #tokenized_ids = [None for i in range(len(sentence_batch))]\n",
    "    ppl = [None for i in range(len(tokenized_ids))]\n",
    "    ppl_lt = [None for i in range(len(tokenized_ids))]\n",
    "    #tokens_lt = [None for i in range(len(tokenized_ids))]\n",
    "        \n",
    "    #print(tokens_lt)\n",
    "    \n",
    "    sen_lengths = [len(x) for x in tokenized_ids]\n",
    "    max_sen_length = max(sen_lengths)\n",
    "    \n",
    "    n_batch = len(tokenized_ids)\n",
    "    input_ids = np.zeros( shape=(n_batch, max_sen_length), dtype=np.int64)\n",
    "    lm_labels = np.full(shape=(n_batch, max_sen_length), fill_value=-1)\n",
    "    \n",
    "    for i, tokens in enumerate(tokenized_ids):\n",
    "        input_ids[i, :len(tokens)] = tokens\n",
    "        lm_labels[i, :len(tokens)-1] = tokens[1:] \n",
    "    \n",
    "    input_ids = torch.tensor(input_ids)#.to(device)\n",
    "    lm_labels = torch.tensor(lm_labels)#.to(device)\n",
    "    \n",
    "    steps = n_batch // bs\n",
    "    print(\"Steps = {}\".format(steps))\n",
    "    if (n_batch % bs == 0):\n",
    "        steps = steps - 1\n",
    "        print(\"Steps = {}\".format(steps))\n",
    "    print(\"Steps = {}\".format(steps))\n",
    "    for i in trange(steps+1):\n",
    "        \n",
    "        if i == steps:\n",
    "            temp_input_ids = input_ids[i * bs : n_batch]\n",
    "            temp_lm_labels = lm_labels[i * bs : n_batch]\n",
    "            temp_sen_lengths = sen_lengths[i * bs : n_batch]\n",
    "        else:\n",
    "            temp_input_ids = input_ids[i * bs : i * bs + bs]\n",
    "            temp_lm_labels = lm_labels[i * bs : i * bs + bs]\n",
    "            temp_sen_lengths = sen_lengths[i * bs : i * bs + bs]\n",
    "            \n",
    "        temp_input_ids = temp_input_ids.to(dvc)\n",
    "        temp_lm_labels = temp_lm_labels.to(dvc)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            lm_pred = lm_model(temp_input_ids)\n",
    "            \n",
    "        loss_val = lm_loss(lm_pred.view(-1, lm_pred.size(-1)), temp_lm_labels.view(-1))\n",
    "        normalized_loss = loss_val.view(len(temp_input_ids),-1).sum(dim= -1) / torch.tensor(temp_sen_lengths, dtype=torch.float32).to(device)\n",
    "        #normalized_loss_lt = loss_val.view(len(temp_input_ids),-1) / torch.tensor(temp_sen_lengths, dtype=torch.float32).to(device)\n",
    "        normalized_loss_lt = loss_val.view(len(temp_input_ids),-1)# / torch.tensor(temp_sen_lengths, dtype=torch.float32).to(device)\n",
    "        tmp_ppl = torch.exp(normalized_loss)\n",
    "        #tmp_ppl_lt = torch.exp(normalized_loss_lt)\n",
    "        ppl[i * bs: i * bs + len(temp_input_ids)] = tmp_ppl.tolist()\n",
    "        ppl_lt[i * bs: i * bs + len(temp_input_ids)] = normalized_loss_lt.tolist()\n",
    "    ppl_lt = [(ppl_lt[i1][:sen_lengths[i1]-1],tokenized_ids[i1][1:]) for i1 in range(len(ppl_lt))]\n",
    "        \n",
    "    \n",
    "    return  ppl, ppl_lt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "def bleu_stats(hypothesis, reference):\n",
    "    \"\"\"Compute statistics for BLEU.\"\"\"\n",
    "    stats = []\n",
    "    stats.append(len(hypothesis))\n",
    "    stats.append(len(reference))\n",
    "    for n in range(1, 5):\n",
    "        s_ngrams = Counter(\n",
    "            [tuple(hypothesis[i:i + n]) for i in range(len(hypothesis) + 1 - n)]\n",
    "        )\n",
    "        r_ngrams = Counter(\n",
    "            [tuple(reference[i:i + n]) for i in range(len(reference) + 1 - n)]\n",
    "        )\n",
    "        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))\n",
    "        stats.append(max([len(hypothesis) + 1 - n, 0]))\n",
    "    return stats\n",
    "\n",
    "def bleu(stats):\n",
    "    \"\"\"Compute BLEU given n-gram statistics.\"\"\"\n",
    "    if len(list(filter(lambda x: x == 0, stats))) > 0:\n",
    "        return 0\n",
    "    (c, r) = stats[:2]\n",
    "    log_bleu_prec = sum(\n",
    "        [math.log(float(x) / y) for x, y in zip(stats[2::2], stats[3::2])]\n",
    "    ) / 4.\n",
    "    return math.exp(min([0, 1 - float(r) / c]) + log_bleu_prec)\n",
    "\n",
    "def get_bleu(hypotheses, reference):\n",
    "    \"\"\"Get validation BLEU score for dev set.\"\"\"\n",
    "    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    for hyp, ref in zip(hypotheses, reference):\n",
    "        stats += np.array(bleu_stats(hyp, ref))\n",
    "    return 100 * bleu(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr=6.25e-5 # original\n",
    "output_dir = \"Path to save the output model\"\n",
    "lr=6.25e-6 \n",
    "warmup=0.002\n",
    "max_grad_norm=1\n",
    "weight_decay=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_len = 90\n",
    "num_train_epochs = 5\n",
    "batch_size = 16\n",
    "beam_size = 5\n",
    "bs = batch_size // beam_size\n",
    "steps = len(input_data) * beam_size // batch_size\n",
    "if len(input_data) * beam_size % batch_size == 0:\n",
    "    steps = steps - 1\n",
    "print(steps, len(input_data), beam_size, batch_size)\n",
    "#steps = len(input_data) // (batch_size * beam_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_input_labels = [x for x in input_data_label for j in range(1 * beam_size)]\n",
    "sent_labls = tokenizer.convert_tokens_to_ids([\"<NEG>\",\"<POS>\"])\n",
    "sents = [sent_labls[x1] for x1 in rollout_input_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "num_train_optimization_steps = len(input_data) * num_train_epochs * beam_size // batch_size\n",
    "optimizer = OpenAIAdam(optimizer_grouped_parameters,\n",
    "                       lr= lr,\n",
    "                       warmup= warmup,\n",
    "                       max_grad_norm= max_grad_norm,\n",
    "                       weight_decay= weight_decay,\n",
    "                       t_total=num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tr_steps, tr_loss, exp_average_loss = 0, 0, None\n",
    "for epoch in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    predictions = [None for i in range(len(input_data))]\n",
    "    prediction_indexes = [None for i in range(len(input_data))]\n",
    "    training_indexes = [None for i in range(len(input_data))]\n",
    "    random_indx = np.random.randint(0, len(input_data), len(input_data))\n",
    "    #input_data = [input_data[random_indx[k1]] for k1 in range(len(input_data))]\n",
    "    #input_data_label = [input_data_label[random_indx[k1]] for k1 in range(len(input_data_label))]\n",
    "    \n",
    "    # Create Trejectories of all the sentences\n",
    "    print(\"Rolling out Trejectories...\\n\")\n",
    "    with open(\"policy_gradient_roll_outs_after_{}_epochs_topp_sampling_dev_cls_lm.txt\".format(epoch),'w') as fp:\n",
    "        for i,x in enumerate(tqdm(input_data)):\n",
    "            #predictions[i], prediction_indexes[i], training_indexes[i] = preditction_with_beam_search(x, beam_width=beam_size)\n",
    "            predictions[i], prediction_indexes[i], training_indexes[i] = top_p_decoding(x)\n",
    "            for z in predictions[i]:\n",
    "                fp.write(z + \"\\n\")\n",
    "    predictions = [y for x in predictions if x != None for y in x] # Generated sentences\n",
    "    prediction_indexes = [y for x in prediction_indexes if x != None for y in x ] # Generated indexes\n",
    "    training_indexes = [y for x in training_indexes if x != None for y in x ] # Indexes to feed for training\n",
    "    pointer_indexes=[(x.index(40481), len(x)-1) for x in training_indexes] # Tuples which points satrt and end\n",
    "    #predictions_for_ppl = [x11 +' '+ y11 for x11,y11 in zip(sents, predictions)]\n",
    "    predictions_for_ppl = [[x11] + y11 for x11,y11 in zip(sents, prediction_indexes)]\n",
    "    \n",
    "    print(\"Classifier Predictions...\")\n",
    "    classifier_preds, classifier_values = run_examples(predictions, dvc=device, bs=64)\n",
    "    classifier_preds_comparison = [x == y for x,y in zip(classifier_preds,rollout_input_labels )]\n",
    "    false_sentences = {i2:predictions[i2] for i2,x in enumerate(classifier_preds_comparison) if not x}\n",
    "    \n",
    "    false_sens = list(false_sentences.values())\n",
    "    false_sens_indexes = list(false_sentences.keys())\n",
    "    #aw, ids, tkns = run_attn_examples(false_sens)\n",
    "    aw, ids, tkns = run_attn_examples(predictions,dvc=device, bs=32)\n",
    "    data1 = prepare_data(aw,ids,tkns)\n",
    "    \n",
    "    attn_words = [None for x in data1]\n",
    "    attn_words_index = [None for x in data1]\n",
    "\n",
    "    for i5, sen in enumerate(data1):\n",
    "        tmp_lt = []\n",
    "        tmp_lt1 = []\n",
    "        for tkns in predictions[i5].split():\n",
    "            if tkns not in sen:\n",
    "                tmp_lt.append(tkns)\n",
    "                tmp_lt1.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tkns)[0]))\n",
    "        attn_words[i5] = (predictions[i5],tmp_lt)\n",
    "        attn_words_index[i5] = tmp_lt1\n",
    "    with open(\"Attention_words_{}_r0_tmp.txt\".format(epoch+1),'w') as fp1:\n",
    "        for sen1, atw in attn_words:\n",
    "            #print(sen1, \" \".join(atw))\n",
    "            fp1.write(sen1 + \"\\tAttributes --> \" + \" \".join(atw) + \"\\n\")\n",
    "            \n",
    "    # prediction_index_with_return\n",
    "    prediction_index_with_return=[[] for i in prediction_indexes]\n",
    "    for i3 in range(len(prediction_indexes)):\n",
    "        if i3 in false_sens_indexes:\n",
    "            for j1 in range(len(prediction_indexes[i3])):\n",
    "                if prediction_indexes[i3][j1] in attn_words_index[i3]:\n",
    "                    prediction_index_with_return[i3].append([-classifier_values[i3],prediction_indexes[i3][j1]])\n",
    "                    #prediction_index_with_return[i3].append((-1,prediction_indexes[i3][j1]))\n",
    "                else:\n",
    "                    prediction_index_with_return[i3].append([0.0,prediction_indexes[i3][j1]])\n",
    "        else:\n",
    "            for j1 in range(len(prediction_indexes[i3])):\n",
    "                if prediction_indexes[i3][j1] in attn_words_index[i3]:\n",
    "                    prediction_index_with_return[i3].append([classifier_values[i3],prediction_indexes[i3][j1]])\n",
    "                    #prediction_index_with_return[i3].append((-1,prediction_indexes[i3][j1]))\n",
    "                else:\n",
    "                    prediction_index_with_return[i3].append([0.0,prediction_indexes[i3][j1]])\n",
    "    \n",
    "    # Calculate Perplexity\n",
    "    _,ppl_lt=calculate_ppl(predictions_for_ppl, dvc=device, bs=32)\n",
    "    for i3 in range(len(prediction_indexes)):\n",
    "        for j1 in range(len(prediction_indexes[i3])):\n",
    "            prediction_index_with_return[i3][j1][0] += (np.exp(-ppl_lt[i3][0][j1]) - 0.1) * 1.2\n",
    "    #print(\"After PPL = {}\".format(prediction_index_with_return))\n",
    "    \n",
    "    # Calculate BLEU Score\n",
    "    bleu_src = [0] * len(r_for_bleu)\n",
    "    c1=0\n",
    "    for x1,x2 in zip(predictions, r_for_bleu):\n",
    "        bleu_src[c1] = get_bleu([x1],[x2])\n",
    "        c1 += 1\n",
    "    #print(\"BLEU Scores = {}\".format(bleu_src))\n",
    "    for i3 in range(len(prediction_indexes)):\n",
    "        for j1 in range(len(prediction_indexes[i3])):\n",
    "            prediction_index_with_return[i3][j1][0] += (bleu_src[i3] * 0.01) * 0.8 \n",
    "    \n",
    "    # Prepare training tensors\n",
    "    training_indexes_tensor = np.zeros(shape=(len(training_indexes),90), dtype=np.int64)\n",
    "    for i, tokens in enumerate(training_indexes):\n",
    "        training_indexes_tensor[i,:len(tokens)] = tokens[:90]\n",
    "    training_indexes_tensor = torch.tensor(training_indexes_tensor)\n",
    "    \n",
    "    for i in trange(steps + 1):\n",
    "        \n",
    "        train_tensor = training_indexes_tensor[i * batch_size : (i * batch_size) + batch_size]\n",
    "        train_pointer_indexes = pointer_indexes[i * batch_size : (i * batch_size) + batch_size]\n",
    "        train_prediction_indexes = prediction_indexes[i * batch_size : (i * batch_size) + batch_size]\n",
    "        train_prediction_indexes_with_return = prediction_index_with_return[i * batch_size : (i * batch_size) + batch_size]\n",
    "\n",
    "            \n",
    "        model.train()\n",
    "        preds = model(train_tensor.to('cuda'))\n",
    "        \n",
    "        # Calculate loss\n",
    "        lt = []\n",
    "        for i1 in range(preds.shape[0]):\n",
    "            count = 0\n",
    "            #print(i1)\n",
    "            for j in range(train_pointer_indexes[i1][0],train_pointer_indexes[i1][1]):\n",
    "                m = Categorical(logits=preds[i1][j % 90])\n",
    "        \n",
    "                temp = torch.tensor(train_prediction_indexes_with_return[i1][count][1]).to('cuda')\n",
    "                #print(m.log_prob(temp).unsqueeze(0))\n",
    "                lt.append(-1 * train_prediction_indexes_with_return[i1][count][0] * m.log_prob(temp).unsqueeze(0))\n",
    "               \n",
    "                count += 1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.cat(lt).mean()\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 500 == 0 :\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "            output_model_file = os.path.join(\"{}\".format(output_dir), \"pytorch_model_zero_grad_{}_{}_v1.bin\".format(epoch, i))\n",
    "            config = model.module.config if hasattr(model, 'module') else model.config\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "    \n",
    "    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "    output_model_file = os.path.join(\"{}\".format(output_dir), \"pytorch_model_zero_grad_{}_final_v1.bin\".format(epoch))\n",
    "    config = model.module.config if hasattr(model, 'module') else model.config\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
